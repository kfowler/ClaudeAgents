# Death Certificate Review Criteria

## Our Standard: Radical Transparency as Competitive Moat

Death certificates aren't corporate PR exercises. They're our competitive advantage through honesty that competitors can't fake. Every certificate must pass ALL criteria or gets rejected.

---

## 1. HONESTY TEST (Pass/Fail)

### ❌ AUTOMATIC REJECTION PHRASES
These kill credibility instantly:
- "Evolved our strategy"
- "Pivoted to better serve users"
- "Strategic realignment"
- "Market conditions changed"
- "Users weren't ready"
- "Ahead of its time"
- "Learning opportunity"
- "Valuable experiment"
- "Iterated based on feedback"
- "Optimized for different outcomes"

### ✅ REQUIRED ELEMENTS

**Plain Language Requirements:**
- Use "We fucked up" not "We encountered challenges"
- Use "Users didn't want this" not "Market adoption was limited"
- Use "We ignored the data" not "We prioritized different metrics"
- Use "We built the wrong thing" not "The solution didn't align with user needs"

**Specific Data Points (minimum 3):**
- Exact usage numbers ("3 invocations in 2 months")
- Real dates ("Warning sign appeared March 15, we ignored until May 1")
- Actual quotes from users ("One user literally said 'Why would I use this?'")
- Time/resource waste ("40 hours that should've gone to payment integration")
- Revenue impact ("$0 revenue, $2000 opportunity cost")

**Ownership Requirements:**
- First person plural ("We misunderstood" not "The market misunderstood")
- Active voice ("We ignored feedback" not "Feedback was not fully incorporated")
- No external blame (market, users, timing, competitors)
- Specific bad decisions ("We chose complexity over simplicity because we wanted to look smart")

---

## 2. EDUCATIONAL VALUE SCORE (8+ Required)

Rate 0-2 points for each category (10 total possible):

### Warning Signs (0-2 points)
- 0: Generic or missing ("usage was low")
- 1: Specific but shallow ("only 5 users in first week")
- 2: Specific with timeline + ignored response ("5 users week 1, we rationalized it as 'early days', kept building for 3 more weeks")

### Decision Points (0-2 points)
- 0: No clear forks in the road
- 1: Mentions decisions but not alternatives
- 2: Shows exact moment + path not taken ("April 3: Could've tested with 5 users, chose to 'perfect' it first")

### Alternative Approach (0-2 points)
- 0: Vague hindsight ("should've validated earlier")
- 1: General direction ("should've done user interviews")
- 2: Specific actionable alternative ("Should've shipped 2-hour MVP, tested with 10 users, THEN decided whether to build full version")

### Pattern Recognition (0-2 points)
- 0: No reusable insight
- 1: Generic pattern ("validate before building")
- 2: Specific pattern others can spot ("When you catch yourself saying 'users will love this WHEN they understand it', you're already fucked")

### Prevention Strategy (0-2 points)
- 0: No concrete prevention plan
- 1: Vague process improvement
- 2: Specific implementable safeguard ("New rule: No agent gets >8 hours investment without 5 user requests")

**Minimum Score: 8/10 or rewrite required**

---

## 3. RESPECT BALANCE (Pass/Fail)

### ❌ INSTANT FAILURES

**Never Mock:**
- Users who tried the agent
- Early adopters who gave feedback
- Team members who built it
- The original vision (even if wrong)

**Never Use:**
- Individual blame ("Sarah's idea")
- Defensive qualifiers ("but we learned a lot!")
- Cynical tone ("another failed experiment")
- Nihilistic endings ("nothing matters anyway")

### ✅ REQUIRED TONE

**Must Include:**
- Thank users by name if possible ("Thanks to @johndoe who tried it anyway")
- Team accountability ("We as a team decided")
- Constructive insights ("Here's what this teaches us")
- Forward momentum ("This failure directly informed our next success")

**Voice Requirements:**
- Confident in failure ("We fucked up and here's exactly how")
- Grateful for lessons ("This expensive mistake saved us from bigger ones")
- Optimistic about future ("Now we know better")
- Professional but human ("We're engineers, not robots")

---

## 4. COMPETITIVE MOAT TEST (Pass/Fail)

### The Certificate Creates Moat If:

**✅ Context Specificity:**
- Mentions our specific tech stack/tools
- References our actual user base
- Includes our unique constraints
- Can't be copy-pasted to another company

**✅ Proprietary Insights:**
- Reveals non-obvious learnings
- Shows counter-intuitive discoveries
- Exposes our specific blind spots
- Teaches what only failure could teach

**✅ Unfakeable Authenticity:**
- Too specific to fabricate
- Too painful to make up
- Too detailed to reverse-engineer
- Too honest for competitors to match

**✅ Competitive Pressure:**
- Makes competitors' vague postmortems look weak
- Sets transparency bar they can't match
- Creates user expectation for honesty
- Builds trust competitors must earn through equal failure

---

## REVIEW PROCESS

### Step 1: Initial Draft
- Technical-writer creates certificate
- Self-review against criteria
- Mark sections that might fail

### Step 2: Critic Review
The-critic evaluates:
1. Honesty Test: Pass/Fail (must pass)
2. Educational Score: X/10 (must be 8+)
3. Respect Balance: Pass/Fail (must pass)
4. Moat Test: Pass/Fail (must pass)

### Step 3: Feedback Loop
If ANY criterion fails:
- Specific line-by-line feedback
- Concrete rewrite suggestions
- Examples of what would pass
- Maximum 2 revision cycles (then escalate)

### Step 4: Approval
All criteria passed:
- Timestamp approval
- Note exceptional elements
- Tag for public release
- Archive for future reference

### Step 5: Post-Publication
Monitor for:
- Community response sentiment
- Competitor reactions
- User trust metrics
- Lessons learned adoption

---

## EXAMPLE REVIEWS

### ❌ REJECTED CERTIFICATE

**Submitted Text:**
> "After careful consideration and user feedback analysis, we've decided to sunset the agent-helper tool. While it provided valuable learnings about agent orchestration patterns, current usage metrics indicate our users' needs have evolved toward more specialized solutions. We're grateful for the journey and excited to apply these insights to future development."

**Critic Feedback:**
```
REJECTED - Complete rewrite required

Honesty Test: FAIL
- "decided to sunset" → Say "killed" or "declared dead"
- "valuable learnings" → What learnings? Be specific
- "users' needs have evolved" → Blame shifting. YOUR understanding evolved
- Zero specific data points
- Zero ownership of failure

Educational Value: 2/10
- No warning signs mentioned
- No decision points identified
- No alternative approach suggested
- No pattern recognition
- No prevention strategy

Respect Balance: FAIL
- Too corporate, not human
- No real gratitude (generic "grateful for journey")
- No team accountability (passive voice throughout)

Moat Test: FAIL
- Could be any company's generic shutdown notice
- No proprietary insights
- Completely fakeable
- Sets no competitive pressure

REWRITE REQUIRED. Start with:
1. What EXACTLY did agent-helper promise?
2. How many people used it? (real numbers)
3. What specific decision killed it?
4. What should you have built instead?
```

### ✅ APPROVED CERTIFICATE

**Submitted Text:**
> "We killed code-reviewer-pro because we built it for ourselves, not our users. Promise: 'AI-powered code review that catches what humans miss.' Reality: 12 invocations in 6 weeks, 3 unique users, 0 paying customers.
>
> The smoking gun: March 15, user @davidchen told us 'I already have GitHub Copilot reviewing my PRs, why do I need this?' We rationalized it as 'he doesn't understand the advanced features.' He understood perfectly - we didn't.
>
> April 2: We had 500 lines of pattern matching code. Could've stopped and asked 10 users if they wanted this. Instead, we added 2000 more lines for 'contextual understanding' nobody requested.
>
> Root cause: We fell in love with the technical challenge (building a better AST analyzer) instead of solving a real problem (developers don't need another code reviewer, they need faster reviews from humans).
>
> Lesson: When you're explaining why users SHOULD want your tool, you've already lost. Users explain to YOU what they need.
>
> Prevention: New rule - No agent gets >1 week investment without 5 users explicitly requesting that specific functionality. Not 'something like it.' That EXACT thing.
>
> Thanks to @davidchen, @mariagarcia, and @alexkim who tried it anyway and gave honest feedback we initially ignored. Your time wasn't wasted - it saved us from wasting 10x more."

**Critic Feedback:**
```
APPROVED - Excellent example of radical transparency

Honesty Test: PASS
- Clear ownership: "we built it for ourselves"
- Specific data: "12 invocations in 6 weeks, 3 unique users"
- Direct language: "We killed" not "discontinued"
- Names the rationalization trap

Educational Value: 9/10
- Warning signs: User feedback on March 15 (specific date)
- Decision point: April 2, could've stopped at 500 lines
- Alternative: Ask 10 users instead of adding 2000 lines
- Pattern: "explaining why users SHOULD want" = failure signal
- Prevention: Specific 5-user rule
(-1 point: Could be more specific about the AST analyzer mistake)

Respect Balance: PASS
- Thanks users by name
- Team owns the failure ("we")
- Constructive tone throughout
- Optimistic close about saved time

Moat Test: PASS
- Specific to code review tooling context
- Reveals non-obvious insight (humans > more AI)
- Too painful and specific to fake
- Makes vague competitor postmortems look dishonest

APPROVED for publication. Exceptional elements:
- Smoking gun quote from user
- Specific line count (500 → 2500)
- Named prevention rule
- Pattern recognition others can use
```

---

## ENFORCEMENT

### Escalation Path
1. First rejection: Technical-writer revises
2. Second rejection: Product-manager assists
3. Third rejection: Full team retrospective on why we can't be honest

### Quality Metrics
Track monthly:
- Rejection rate (target <20%)
- Average educational score (target 8.5+)
- Community trust score (via surveys)
- Competitor transparency changes (are they following our lead?)

### Continuous Improvement
Quarterly review of:
- Are criteria too lenient? (check for corporate speak creeping in)
- Are we learning from our honesty? (track prevention rule adoption)
- Is it building competitive moat? (monitor competitor responses)

---

## REMEMBER

Every death certificate is a trust deposit with our users. Every corporate euphemism is Aa withdrawal. We're building a trust surplus that competitors can't match without equal honesty about equal failure.

The bar is high because the payoff is higher: undeniable authenticity in a world of corporate bullshit.

Hold. The. Line.